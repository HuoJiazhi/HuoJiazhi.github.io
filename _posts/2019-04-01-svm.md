---
layout: post
title:  "SVM学习记录"
crawlertitle: ""
summary: "SVM学习记录"
date:   2018-09-10 12:09:47 +0700
categories: 
tags: 'MachineLearning'
author: Jiazhi
---
*最近又看了一些关于支持向量机的知识，记录一下自己的学习心得，总的来说SVM还是非常好的分类器算法，能够找到一个关于问题的最优解，但是面临一个很大的问题就是很多时候可能会无法找到合适的核函数去解决问题，其实整个SVM的学习个人推荐b站一个up主的讲解视频（av28186618），讲解整个算法从Hard-Margin到核函数再到Soft-Margin都很清楚，但是还是缺少我比较想学习的SMO算法，但是其实这个算法的实现对于SVM来说不是太重要的核心知识，李航老师和周志华老师的书中都介绍得比较浅，机器学习实战中有代码可以去看看。它是一个解决规划问题的解法而已，所以个人认为不完全理解原理也是可以的。*

**Hard-Margin**

首先SVM是一个判别模型，区别于贝叶斯的生成模型，其实区分两个模型最直接的方法就是，判别模型是估计概率P(x|y)，而生成模型是估计P(x,y)的概率，然后Margin的意思就是离超平面最近的点和超平面之间的距离，SVM的目的就是使整个Margin变大，因为最大的Margin可以使分类的情况变大最好。如下图所示，其实我们的W的不同，点与直线的距离也会随之等距离扩大的。道理就是4x<sub>1</sub> = 2x<sub>2</sub> +4 和2x<sub>1</sub> =x<sub>2</sub> +2 所表达的直线是一样的，但是一个点和他们的距离根据距离公式运算出来的大小是不一样的。所以我们就可以将这个最小的距离做一个规定，令他等于1。这个做法可以方便后面的运算。这样我们就可以得到关于这个问题的一个规划限制和目标值。如下图所示：
之后我们就可以利用拉格朗日乘子法构造一个与之对应的对偶问题。

------------



------------


